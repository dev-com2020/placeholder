version: 3
created_by: "tomaszk"
domain: "technology"

document_outline: |
  Krótki opis TinyLlama, wariantów kwantyzacji i przykładowych zastosowań.

document:
  repo: "https://github.com/dev-com2020/placeholder"
  commit: "main"
  patterns:
    - "docs/placeholder.md"

seed_examples:
  - context: |
      TinyLlama to mały model językowy zoptymalizowany pod słabszy sprzęt.
    questions_and_answers:
      - question: Do czego najlepiej użyć TinyLlama?
        answer: Do prostych zadań jak Q&A, krótkie podsumowania czy transformacje tekstu.
      - question: Czy działa na CPU?
        answer: Tak, TinyLlama działa na CPU, choć GPU przyspiesza generowanie.
      - question: Czy sprawdzi się w długich analizach?
        answer: Nie, do tego lepsze są większe modele.

  - context: |
      Kwantyzacja zmniejsza rozmiar modelu i zapotrzebowanie na RAM kosztem jakości.
    questions_and_answers:
      - question: Po co stosuje się kwantyzację?
        answer: Żeby model działał na słabszym sprzęcie, z mniejszym użyciem RAM.
      - question: Czym różni się Q3_K od Q4_K_M?
        answer: Q4_K_M zwykle daje lepszą jakość, ale wymaga więcej pamięci.
      - question: Czy kwantyzacja zawsze obniża jakość?
        answer: Tak, ale skala spadku zależy od wariantu.

  - context: |
      TinyLlama jest lekki, więc dobrze nadaje się do użycia w pipeline'ach lokalnych.
    questions_and_answers:
      - question: Dlaczego TinyLlama jest popularny lokalnie?
        answer: Bo działa szybko na zwykłych komputerach bez dużych GPU.
      - question: Czy nadaje się do prywatnych projektów?
        answer: Tak, bo nie wymaga wysyłania danych do chmury.
      - question: Jakie aplikacje lokalne mogą go używać?
        answer: np. notatniki, chatboty, proste asystenty.

  - context: |
      Wybór kwantyzacji to kompromis między szybkością a jakością.
    questions_and_answers:
      - question: Co wybrać na start?
        answer: Najczęściej Q4_K_M, bo jest dobrym kompromisem.
      - question: Kiedy użyć Q2_K?
        answer: Gdy masz bardzo mało RAM i akceptujesz spadek jakości.
      - question: A kiedy Q5_K lub Q6_K?
        answer: Gdy zależy Ci na jakości i masz wystarczająco pamięci.

  - context: |
      TinyLlama nie nadaje się do bardzo złożonych zadań.
    questions_and_answers:
      - question: Jakie ograniczenia ma TinyLlama?
        answer: Krótki kontekst, słabsze rozumowanie niż duże modele.
      - question: Do jakich zadań nie używać?
        answer: Do długich analiz, kodu i skomplikowanych workflowów.
      - question: Jak poprawić jakość bez zmiany modelu?
        answer: Lepsze prompty, krótszy kontekst lub użycie RAG.
